---
title: "Tarea4"
author: "Javier Montiel González, Eliza Zenteno Munuzuri, Andrés Cruz y Vera"
date: "24/10/2020"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Ejercicio 1

Matriz de correlaciones
```{r echo=FALSE}
rho<-matrix(c(1, 0.63, 0.45,0.63,1,0.35,0.45,0.35,1),3)
rho
```

Al aplicar el método de máxima verosimilitud del análisis de factores con m=1 obtenemos lo siguiente:
```{r, echo=FALSE}
f<-factanal(covmat=rho, factors=1)
f
#Vector de cargas del factor 1
L<-f$loadings
#Matriz de varianzas de los errores
psi<-diag(f$uniquenesses)
```
Calculamos $LL'+\Psi$ 

```{r}
L%*%t(L)+psi
```

Por lo tanto, $\rho=LL'+\Psi$, salvo por errores numéricos.



# Ejercicio 2 
Se tiene la siguiente matriz de factores no rotada, obtenida utilizando el método de componentes principales y considerando 4 factores.
```{r}
library(stats)
factores <- matrix(c( 0.881, 0.828, 0.664, 0.792, 0.731, 0.476, -0.347, 0.508, -0.711, 0.564, -0.647, 0.804, -0.165, -0.070,0.154, -0.179, 0.117, 0.329, 0.268, -0.200, -0.031, -0.029, -0.125, 0.135),nrow=6)
row.names(factores) <- paste0("X",1:6)
colnames(factores) <- paste0("F",1:4)
factores
```
Obtener las comunalidades, el eigenvalor y el porcentaje de variación explicada

comunalidades: 
```{r}
diag(factores%*% t(factores))
```
Los altos valores de la comunalidades indican que los 4 factores considerados explican una buena proporcion dela varianza de cada variable. 

```{r}

```

# Ejercicio 3 
a)$(I + L^T \psi L)^{-1} (L^T \psi^{-1}L)$ = $I - (I + L^T \psi^{-1}L)^{-1}$


$(I + L^T \psi L)^{-1} (L^T \psi^{-1}L)$ = $(I + L^T \psi L)^{-1} (I + L^T \psi^{-1}L -I)$ = $(I + L^T \psi L)^{-1} ((I + L^T \psi^{-1} L) -I)$ =  $(I + L^T \psi L)^{-1}  (I + L^T\psi^{-1}L) - (I + L^T \psi^{-1}L)^{-1}I$ = $I - (I + L^T \psi^{-1}L)^{-1}$


b) $L^{T}(LL^{T} + \psi)^{-1}$ = $(I + L^T \psi^{-1} L)^{-1} L \psi^{-1}$


$L^{T}(LL^{T} + \psi)^{-1}$ = $L^{T}(LL^{T})^{-1} + L^T\psi^{-1}$ 
 = $L^{-1} + L^T \psi^{-1}$
 = $(L^{-T} \psi L^{-1} + I) L^T \psi^{-1}$ = $(I + L^T \psi^{-1} L)^{-1} L \psi^{-1}$

# Ejercicio 4. 
El siguiente ejemplo muestra un caso que se conoce como el caso de Heywood. Consideren
un modelo factorial con m = 1 para la población con matriz de covarianza $\Sigma$ 
```{r}
c1 <-c(1,0.4,0.9)
c2 <-c(0.4, 1, 0.7)
c3 <-c(0.9, 0.7, 1)

rbind(c1,c2,c3)
```


Considerando $L = [l_{1,1}, l_{1,2}, l_{1,3}]'$

A partir de la matriz $\sum$ se definen las siguientes ecuaciones: 
(1) $1 = l_{1,1}^2 + \psi_1$ (2) $0.4 = l_{1,1}l_{1,2}$  (3) $1 = l_{1,2}^2 + \psi_2$                      (4) $0.9 = l_{1,1}l_{1,3}$  (5) $0.7 = l_{1,2}l_{1,3}$
                 (6) $1 = l_{1,3}^2 + \psi_3$

Despejando $l_{1,1}$ de (1), sustituimos en (2) y (4) de donde se obtiene que: $l_{1,2} = \frac{0.4}{\sqrt(1-\psi_1)}$ , $l_{1,3} = \frac{0.9}{\sqrt(1-\psi_1)}$. Sustituyendo ambos valores en (5) se obtiene que $\psi = 0.48572$

Resulta que: 
$l_{1,1} = +/- 0.7171$    $\psi_1 = 0.4857$
$l_{1,2} = +/- 0.5577$    $\psi_2 = 0.6888$
$l_{1,3} = +/- 1.2549$    $\psi_3 = -0.5750$

Ocurre que $\psi_3 \leq 0$ pero no es valido ya que se trata de una varianza. 


# Ejercicio 5
Datos de monitoreo atmosférico (REDMA)

Para la creación de la base de datos se tomaron los regitros de las estaciones que midieran los tres contaminantes (PM10, PST y PM25) considerados para el año 2019. Además, se descartaron las estaciones que tuvieran mediciones nulas en todos sus registros del año para alguno de los contaminantes. Así pues, las tres estaciones que se escogieron fueron: Tlanepantla (TLA), UAM Iztapalapa (UIZ) y Xalostoc (XAL). Se junto toda la información en una sola base con los siguientes campos: FECHA, ESTACIÓN, PM10, PST y PM25, con un toral de 151 observaciones.

Calculamos la correlación de las variable consideradas (PM10, PST y PM25)

```{r echo=FALSE}
library(readxl)
library(dplyr)
library(corrplot)
#Leemos la base 
datos <- read_excel("./BD.xlsx")
#Asignamos los valores nulos
datos <- na_if(datos,-99)
#Eliminamos los registros con alguna una medicion nula
datos <- na.omit(datos)
cont<-datos[-c(1,2)]
#Calculamos la matriz de covarianzas y correlacion
S<-cov(cont)
R<-cor(cont)
#Grafica de dispersion
#plot(cont)
#Grafica de correlacion
corrplot(R, method = "ellipse")
```

Nótese que las variables PM10 y PST están altamente correlacionadas y en un menor grado PM10 y PM25, por lo que se podrían tener hasta dos factores.

Aplicamos el método de máxima verosimilitud a la matriz de correlaciones y notamos que el método solo soporta un factor.
```{r echo=FALSE}
m<-factanal(covmat= R, factors=1)
m
```

Observamos que la proporción de la varianza explicada por el modelo es de 0.91. Así pues, con un factor parece ser suficiente para describir las variables. Las cargas asignadas nos sugieren que el factor asigna pesos similares a PM10 y PST con PM25 casi una decima menos a PST. Dicho factor podría considerarse como la contaminación del aire en la CDMX.

Por otro lado, al aplicar el método de componentes principales a la matriz de covarianzas obtenemos lo siguiente:

```{r echo=FALSE}
eigenS<-eigen(S)
#Varianza explicada por un factor
v<-eigenS$values[1]/sum(eigenS$values)
#Scores
L1<- round(eigenS$vectors[,1]*sqrt(eigenS$values[1]),2)
names(L1)<-c("PM10", "PST", "PM25")
L1
```

La varianza explicada por el primer eigenvalor es de 0.9783, por lo que tomar un factor es razonable para describir a las variables. Resulta que en este caso las cargas tienen valores negativos y la variable PST tiene un mayor peso. En este caso, el factor podría considerarse como la calidad del aire, la cual es menor cuando hay una mayor concentración de particulas contaminantes en el aire.

Calculamos los scores:

#Máxima Verosimilutud
```{r echo=FALSE}
m1 <- factanal(x = cont, factors = 1, scores = "Bartlett")
head(m1$scores)
```

#Componentes principales
```{r echo=FALSE}
Psi<-diag(diag(S)-diag(L%*%t(L)))
mu <- colMeans(cont)
f <- NULL
M <- as.matrix(datos[-c(1,2)])

for (i in 1:length(datos$FECHA)){
  f[i] <- solve(t(L)%*%L)%*%t(L)%*%solve(Psi)%*%(M[i,]-mu)  
}

head(f)
```


```{r echo=FALSE}
plot(datos$FECHA, m1$scores, main="Máxima verosimilitud")
```

Observamos que de acuerdo al índice de contaminación que se creo, las estaciones monitorean una mayor contaminación en invierno con valores distintos en cada una y conforme se acerca la primavera la contaminación disminuye. Lo anterior es consistente con un fénomeno físico conocido como inversión térmica.

```{r echo=FALSE}
plot(datos$FECHA, f, main="Componentes Principales")
```

Por otro lado, para el índice obtenido con la matriz de covarianzas obtenemos un comportamiento inverso. Dado que se está considerando el factor como la calidad del aire, tiene sentido que tenga ese comportamiento.

# Ejercicio 6

Contamos con la siguiente matriz de correlaciones

```{r , echo=FALSE}
R <- matrix( c(1, 0.615, -0.111, -0.266,
               0.615, 1, -0.195, -0.085,
               -0.111, -0.195, 1, -0.269,
               -0.266, -0.085, -0.269, 1), nrow = 4, byrow=TRUE)
R
corrplot(R, method="ellipse")
```



Ahora calcularemos las correlaciones canonicas construyendo la matriz A y calculando los valores propios



```{r}

R11 <- R[1:2, 1:2]
R12 <- R[1:2, 3:4]
R21 <- R[3:4, 1:2]
R22 <- R[3:4, 3:4]

(A <- solve(R11) %*% R12 %*% solve(R22) %*% R21)
eigen(A)

(B <- solve(R22) %*% R21 %*% solve(R11) %*% R12)
eigen(B)
```
a.
Así, encontramos las correlaciones canónicas muestrales que corresponden a la raíz cuadrada de los valores propios $\sqrt{0.1067}=0.327$ y $\sqrt{0.0293}=0.17$

b. EL primer par canónico es:

$$U_1 = 0.0450 X^{(1)}_1 + 0.0256X^{(1)}_2$$
$$V_1 = -0.524 X^{(2)}_1  -0.851X^{(1)}_2$$

Notamos que la primer variable canónica le da más peso a los homicidios no primarios y la segunda variable le da mayor importancia a la probabilidad de castigo.Consideramos que las variables representan índices sobre la composición de los homicidios así como de las características de la acción punitiva. Además, la segunda variable canónica tiene signos negativos pues el segundo grupo de variables se correlacionan de forma negativa con el primer grupo. La correlación canónica es relativamente pequeña.

# Ejercicio 7

Primero cargaremos los datos

```{r  }
data <- read.csv("./bank-full.csv", header=T, sep=";")
```

Y obtenemos un primer resumen de los datos

```{r, echo=FALSE}
str(data)
```

```{r, echo=FALSE}
as.matind <- function(z, colname) { #crea una matriz de indicadoras, z es categorica
  z <- as.factor(z)
  l <- levels(z)
  b <- as.numeric(z==rep(l,each=length(z)))
  matind <- matrix(b, length(z))
  colnames(matind) <- paste0(paste(colname,"."), l)
  return(matind)
}
```

Nuestra primera idea fue tranformar las variables de tipo factor a indicadores y codificar las variables binarias (con valores "yes" o "no") con ceros y unos. Despues de probar con cada una de las variables categóricas (job, marital, education, contact, poutcom) nos encontramos con el problema de que al intentar calcular las correlaciones canónicas , teníamos matrices numericamente singulares. Inclusive invocando la funcion stats::concor, la ejecución nos regresaba una dirección canónica con coeficientes faltantes.

Para poder avanzar tuvimos que dejar de lado las variables categoricas y realizar el analisis con las variables restantes.Así, trabajamos con los siguientes datos transformados.

```{r, echo=F}
numeric_vars <- c(1,6,10,12,13,14,15)
datac <- data[, numeric_vars]
#binary_vars <- c(5, 7, 8)
binary_vars <- c( 5,7,8)
#categorical_vars <- c(2,3,4,9,11,16)
categorical_vars <- c()
for (var in binary_vars ){
  datac <- cbind(datac, as.numeric(data[, var] == "yes"))
  colnames(datac)[ncol(datac)] <- colnames(data)[var]
}

for( var in categorical_vars){
 datac <- cbind(datac, as.matind(data[, var], colnames(data)[var]))
}

datac <- cbind(datac, y=as.numeric(data[, 17] == "yes"))
n_vars <- ncol(datac) - 1
r <- cor(datac)
str(datac)
corrplot(r, method="ellipse")

```


Ahora que tenemos una representación más amigable de los datos, consideramos que para atacar el problema de clasificación procederemos a realizar el análisis de correlación canonica tomando a "y" como uno de los grupos y las demas variables en el otro grupo. Esto se realiza con la intención de utilizar las variables canónicas como sustitutas que permitan representar la relación entre ambos grupos de variables de forma simplificada. 

```{r, echo=F}
# ca <- function(R, n_vars){
#   R11 <- R[1:n_vars, 1:n_vars]
#   R12 <- R[1:n_vars, n_vars+1]
#   R21 <- R[n_vars +1, 1:n_vars]
#   R22 <- R[n_vars+1, n_vars+1]
#   
#   (A <- solve(R11) %*% R12 %*% solve(R22) %*% R21)
#   eigen(A)
#   
#   (B <- solve(R22) %*% R21 %*% solve(R11) %*% R12)
#   eigen(B)
# }
# 
# ca(r, n_vars)
```

```{r, echo=F}     
(cca <- stats::cancor(x = datac[,1:(n_vars)], y = datac[,n_vars+1], ycenter =F))
```


```{r, echo=F}
ind_res <- 1
primer_var_canonica_regresores <- as.matrix(datac[,1:n_vars]) %*% cca$xcoef[,1]
primer_var_canonica_y <- datac[,n_vars] %*% cca$ycoef
plot(primer_var_canonica_regresores, primer_var_canonica_y, xlab = "Primer variable canónica del grupo de regresores", ylab = "Primer variable canónica de y")
```

Las variables canónicas pueden ser utilizadas en un modélo posterior que defina un nivel de corte para la clasificación dentro de las personas que compran o no el servicio


# Ejercicio 8
Se tienen tres medidas fisiológicas y tres variables de ejercicios medidas en 20 hombres de
30-40 años en un gimnasio. Los datos están en el archivo FitnessClubdata.dat .
Objetivo: determinar si las variables fisiológicas se relacionan de alguna forma con las
variables de ejercicio.
 
a. Analizar la matriz de correlaciones relevantes entre las variables de los dos grupos
(dentro y entre grupos de variables).
```{r}
library(corrplot)
datos <- read.table("FitnessClubData.dat", header = TRUE)

#Calculamos la matriz correlacion por grupo de variables
#para variables fisiologicas  
R1<-cor(datos[,1:3])
#Grafica de correlacion
corrplot(R1, method = "ellipse")
#para variables de ejercicio 
R2<-cor(datos[,4:6])
#Grafica de correlacion
corrplot(R2, method = "ellipse")
```

Para las variables fisiológicas, vemos que hay una alta correlación positiva entre las medidas de cintura y peso, sin embargo el pulso tiene una correlación negativa pero no muy marcada con respecto a las otras variables. 

Se podría decir que una persona de mayor peso tendrá una mayor medida de cintura, y en general un pulso menor a comparación con una persona de menor peso, pero esta diferencia 
en el pulso probablemente no será muy marcada. 

En las variables de ejercicio vemos que las correlciones son altas y positivas. En general una persona con buena capacidad de hacer lagartijas tiene igual una buena capacidad de llevar a cabo sentadillas o saltos. Se podrían tomar la relación entre las 3 variables como un un indicador de buena capacida física. 

```{r cars}
#Calculamos la matriz correlacion entre todas las variables
R<-cor(datos)
#Grafica de dispersion
#plot(cont)
#Grafica de correlacion
corrplot(R, method = "ellipse")
```

Al analizar la correlacion incluyendo variables tanto fisiologicas como de ejercicio vemos que las variables de ejercicicio tienen una correlación negativa con la varible de peso, se podría tomar como un indicador que de que a mayor peso se suele tener un menor rendimiento físico. La cintura vimos se correlaciona de forma positiva con el peso por lo que esta variables igual tiene correlaciones negativas con las variables de ejercicio. 
La variable pulso tiene una correlación negativa con el resto de las variables fisiológicas por lo que tiene una correlación positiva, con las variables de ejercicio aunque no es una correlación fuerte.  


